---
title: "QSG 11/18"
author: "Andrew Villeneuve"
date: "11/12/2019"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(tinytex.verbose = TRUE)
library(knitr)
library(rmarkdown)
library(lme4)
library(ggplot2) 
library(MuMIn)
library(raster)
library(dplyr)
library(gdata)
library(logistf)
library(data.table)
library(stringr)
library(PerformanceAnalytics)
library(bsts)
library(brglm)
library(AICcmodavg)
library(plyr)
library(extrafont)
library(here)
loadfonts(device = "win")
 

lt50<- function(model,value) {
    function(x) {
      predict(model,data.frame(t5=x),type="response")-value}}

# Given a model, predict values of yvar from xvar
# This supports one predictor and one predicted variable.
# xrange: If NULL, determine the x range from the model object. If a vector with
#   two numbers, use those as the min and max of the prediction range.
# samples: Number of samples across the x range.
# ...: Further arguments to be passed to predict()
predictvals <- function(model, xvar, yvar, xrange=NULL, samples=100, ...) {

  # If xrange isn't passed in, determine xrange from the models.
  # Different ways of extracting the x range, depending on model type
  if (is.null(xrange)) {
    if (any(class(model) %in% c("lm", "glm")))
      xrange <- range(model$model[[xvar]])
    else if (any(class(model) %in% "loess"))
      xrange <- range(model$x)
  }

  newdata <- data.frame(x = seq(xrange[1], xrange[2], length.out = samples))
  names(newdata) <- xvar
  newdata[[yvar]] <- predict(model, newdata = newdata, ...)
  newdata
}
here()
data<-read.csv(here::here("data/master.copy(9.17.19).csv"))
runs<-read.csv(here::here("data/run.count.csv"))
#setwd("C:/Users/drewv/Documents/UMASS/data")
#data<-read.csv("master.copy(9.17.19).csv")
#runs<-read.csv("run.count.csv")
data$acc<-as.factor(data$acc)
data$survival<-as.character(data$survival)

which(data$weight>0.007)
data[253,7]<-.0017
which(data$t1>60)
data[474,9]<-24.8
data[504,9]<-24.8
data[534,9]<-24.8
which(data$t2<10)
data[962,10]<-25.5
data[992,10]<-25.5
data[1022,10]<-25.5
which(data$t5<10)
data[962,13]<-29.3
```
### The Study
The purpose of the $CT_{max}$ study was to evaluate the thermal tolerances (or $CT_{max}$) of *Urosalpinx cinerea* from a range of populations across latitude in both the Atlantic and Pacific Oceans. Further, these populations of snails were incubated, hatched, acclimated at two different temperatures (20째 and 24째 C), to test for plasticity in thermal tolerances across population. We expect populations from northern "cold" sites to show lower $CT_{max}$ then populations from southern "warm" sites.

## Metadata

# What is a run!
A "run" is a sequence of 15-30 snails for a given population and acclimation. Each "run" has no column replicates, so if we did not randomly distribute them by row they would be in a single row. A heatbar run is when up to three different "runs" are run simultaneously. 

# Date
The date is the date of the heatbar "run" (as defined below) for a given population and acclimation. 

# Site
Two-letter abbreviation of sites used. Correspond to the following sites:

```{r}
site.meta<-data.table(siteID = c("GB","WH","OY","BF","FB","HM","TO"),site=c("Great Bay","Woods Hole","Oyster","Beaufort","Folly Beach","Humboldt","TO"),state=c("NH","MA","VA","NC","SC","CA","CA"))
site.meta
```

# Acc
Acc is the acclimation temperature. Each population was reared in either 20째C or 24째C seawater (30PSU). We are interested in knowing if acclimation temperature will expose signs of plasticity in thermal tolerance for each population.

# col
Col is the column in which the individual from that particular run/population/acc was placed. For both protocols 1 and 2, all snails were placed in positions 9-38. No snails were in 1-8 due to the low temperature in these positions closeest to the cooling element. 

# rand
a random number generated to assign column positions to snails

# row
The row (A,B,C) randomly assigned for each run's snail

# weight
The dry weight of each snail before being used in the heatbar experiment.Snails were dabbed dry to remove excess water on and inside the shell.

# T0-T5
These are the timepoints at which temperatures were taken. Each has a step in between of 1 hr. The table below shows the ramping rate on the PID controller (setpoints) in relation to time and the timepoints at which temperature was measured. 

# Survival
Survival of the snails was established between 15-18 hrs after the experiment end. Tubes of the snails were floated in their acclimation temperature seawater immediately after the experiment for the 15-18 hours. Afterwards, each snail was examined under a microscope and prodded with a probe. If the foot retracted, they were scored as "alive" (or 1), and if not, they were scored as "dead" (or 0). In at least one run, some snails were discovered to have been drilled (cannibalized) before the experiment. These were classed as "d" for drilled, and were not included in analysis. Future runs had each snail examined for signs of drilling before experiment ran. In the run with drilled snails, I counted snails that had tissue in the shell but clearly dead as "0"s.

# Protocol
Three total protocols were tested in the experiment, with only two included in data analysis. Most runs were completed under protocol 1, wherein temperature was ramped from 25C to 60C and ice added every hour for the first two hours of the five hour experiment. Protocol 2 had the same ramping rate but had no ice added. Protocol 3 had no ice added and had a lower end setpoint of 50C. P1 had a very wide range in temperature, and so we developed P2 to lessen the spread between temperature positions. P3 was developed for the same reason, but the projected CTmax point was too close to the heating element (not enough "0"s for the model).


```{r,include=F}
ramp<-data.table(time_hrs  = c("0","0.5","1","1.5","2","2.5","3","3.5","4","4.5","5"),timepoint=c("T0","","T1","","T2","","T3","","T4","","T5"),setpoint_P1P2=c("20/24","25","30","35","40","45","50","55","60","60","60"),P1_ice=c("yes","no","yes","no","yes","no","no","no","no","no","no"),P2P3_ice=c("no","no","no","no","no","no","no","no","no","no","no"),setpoint_P3=c("20/24","25","28","31","34","38","42","46","50","50","50"))
```
```{r,include=T}
ramp
```

In the first heatbar run, I included cannibalized snails in the heatbar. I was able to differentiate at the end of the heatbar run which snails had died because of cannibalisim by looking for drill holes on the shells. These snails were denoted with a "d". Snails which had no drill holes and still had the body of the snail within the shell were counted as mortalities in the heatbar. 

I also need to get rid of protocol 2 runs, as the ramping rate was too dissimilar to protocol 1. 

Finally, not every column in the heatbar was used, and so these are blank rows we can get rid of.

```{r,include=F}
data<-filter(data, survival != "d") #get rid of "drilled" uros 
data<-filter(data, protocol != 3) #get rid of protocol 2
data<-filter(data,run_count>13)
data<-filter(data, weight != "NA")

data = filter(data,site!="BFII")
data$site[data$site=="FB2"] <- "FB"
data$site[data$site=="OYII"] <- "OY"
data$site[data$site=="TO2"] <- "TO"
data = filter(data,site!="FB2")
data = filter(data,site!="OYII")
data = filter(data,site!="TO2")

data$site<-factor(data$site)
data$survival<-as.numeric(data$survival)
#data$survival<-factor(data$survival)


table(data$survival, data$site, data$acc)
tapply(data$survival, data$site, length)
pop<-tapply(data$survival, list(data$site,data$acc), length)
```
Data is now fully cleaned! Time to do some exploration.How well did we sample all the sites across acclimation temperatures?

```{r,include=T}
ggplot(data,aes(x=site, fill=acc))+geom_bar(position="dodge")+facet_wrap(protocol~.)
```
## Visualization of all data

```{r, include=F}
my.xlab = expression(paste("Temperature"," (",degree,"C)"))
LT.plot<-ggplot(data, aes(x=t5,y=survival, color=acc))+
  geom_point(alpha=1/2,size=4,position=position_jitter(width=0,height=.03))+(facet_wrap(protocol~.))+
  theme_bw()+ylab("Survival")+xlab(my.xlab)+
  theme(axis.text=element_text(size=18),axis.title.x=element_text(size=22,vjust=-.1),
        panel.grid.major=element_blank(),panel.grid.minor=element_blank(),axis.title.y =element_blank())+
  scale_x_continuous(breaks=c(10,15,20,25,30,35,40,45,50,55,60,65))+scale_y_discrete(breaks=c(0,.5,1))
```
```{r,include=T}
LT.plot
```
```{r, include=F}
LT.plot2<-ggplot(data, aes(x=t5,y=survival, color=acc))+
  geom_point(alpha=1/2,size=4,position=position_jitter(width=0,height=.03))+
  theme_bw()+ylab("Survival")+xlab(my.xlab)+
  theme(axis.text=element_text(size=18),axis.title.x=element_text(size=22,vjust=-.1),
        panel.grid.major=element_blank(),panel.grid.minor=element_blank(),axis.title.y=element_blank())+scale_x_continuous(breaks=c(10,15,20,25,30,35,40,45,50,55,60,65))+scale_y_discrete(breaks=c(0,.5,1))+facet_grid(site~protocol)


```
```{r,include=T}
LT.plot2


summary(glm(data=data,survival~weight,family="gaussian"))
ggplot(data=data,aes(x=survival,y=weight,group=survival))+geom_boxplot()
summary(aov(survival~weight,data=data))
```

### Individual Analysis
Now that I have a workflow, I will start off by producing binary models and extracting $CT{max}$ from each run. This is so I can make a for-loop later to perform this over all runs. 

```{r,include=T,echo=F}
run1<-filter(data,run_ID == 1)

t5<-run1$t5


m1<-brglm(data=run1,survival~t5,family="binomial")
summary(m1)

c0=coef(m1)[1] #model intercept 
c1=coef(m1)[2] #model slope 

p1=plogis(c0+c1*t5)
comb<-data.frame(t5,p1)


p<-ggplot(run1, aes(x=t5,y=survival))+
  geom_point(alpha=1/2,size=4,position=position_jitter(width=0,height=.03))+
  theme_bw()+ylab("Survival")+xlab(my.xlab)+
  theme(axis.text=element_text(size=18),axis.title.x=element_text(size=22,vjust=-.1),
        panel.grid.major=element_blank(),panel.grid.minor=element_blank(),axis.title.y =element_blank())+
  scale_x_continuous(breaks=c(10,15,20,25,30,35,40,45,50,55,60,65))+scale_y_discrete(breaks=c(0,.5,1))+geom_line(data=comb,aes(x=t5,y=p1))

a1<-uniroot(lt50(m1,0.5),range(run1$t5))$root
a1

p+geom_hline(yintercept=.5)+geom_vline(xintercept=a1)
```

Here we can see how I will analyze each "run." Note that there is an outlier death. When removed, we see that there is a change in the ultimate LT50. This builds a case for removing clear outliers.

```{r,include=F}
run1<-filter(data,run_ID == 1)
run1<-run1[-c(1),]



t5<-run1$t5


m1<-brglm(data=run1,survival~t5,family="binomial")
summary(m1)

c0=coef(m1)[1] #model intercept 
c1=coef(m1)[2] #model slope 

p1=plogis(c0+c1*t5)
comb<-data.frame(t5,p1)


p<-ggplot(run1, aes(x=t5,y=survival))+
  geom_point(alpha=1/2,size=4,position=position_jitter(width=0,height=.03))+
  theme_bw()+ylab("Survival")+xlab(my.xlab)+
  theme(axis.text=element_text(size=18),axis.title.x=element_text(size=22,vjust=-.1),
        panel.grid.major=element_blank(),panel.grid.minor=element_blank(),axis.title.y =element_blank())+
  scale_x_continuous(breaks=c(10,15,20,25,30,35,40,45,50,55,60,65))+scale_y_discrete(breaks=c(0,.5,1))+geom_line(data=comb,aes(x=t5,y=p1))

a1<-uniroot(lt50(m1,0.5),range(run1$t5))$root
```
```{r,include=T,echo=F}
a1

p+geom_hline(yintercept=.5)+geom_vline(xintercept=a1)
```
Below is a loop I used to extact LT50s by run and store in a dataframe. 
```{r,include=F}
##data<-data[-c(652),]
##data<-data[-c(652),]
data<-data[-c(1),]
##data<-data[-c(35),]
##data<-data[-c(35),]
#data<-data[-c(524),]
#data<-data[-c(669),]
#data<-data[-c(748),]

#View(which(data$t5<34&data$survival==0))
data<-filter(data,run_ID !=3)
#The above observations and run are problematic and eliminated.
```
```{r,include=T,warning=F}
##forloop

runs<-unique(data$run_ID)
df<-list()
for (i in 1:length(runs)){
  tmp<-filter(data,run_ID==runs[i])
  df[[i]]<-tmp
 
}

modlist<-list()
for (i in 1:length(df)){
  mod<-brglm(data=df[[i]],survival~t5,family="binomial")
  modlist[[i]]<-mod
}
names(modlist)<-runs

LT50<-list()
for(i in 1:length(modlist)){
  lt<-dose.p(modlist[[i]],p=0.5)[1]
  LT50[[i]]<-lt
}

dflt<-data.frame(LT50,ncol=length(LT50))
dflt<-melt(as.data.table(dflt))
dflt<-dflt[-c(31),]#32 if we keep run_ID ==3
dfdlt2<-data.frame(dflt,runs)
lt50s<-subset(dfdlt2,select=-c(variable))
names(lt50s)<-c("LT50","run_ID")
#View(lt50s)

run.count <- read.csv("~/UMASS/data/run.count.csv")
lt50s<-merge(lt50s,run.count,by="run_ID")
```
I've now extracted all of my $CT{max}$ values. I just need to merge this with other metadata for each run so that I can analyze by latitude. 
```{r,include=F}
lt50s$lat<-ifelse(lt50s$site=="BF",34.819,ifelse(lt50s$site=="FB",32.660525,ifelse(lt50s$site=="GB",43.089589,ifelse(lt50s$site=="HM",40.849448,ifelse(lt50s$site=="OY",37.288562,ifelse(lt50s$site=="TO",38.12805,ifelse(lt50s$site=="WH",41.57687,NA)))))))
lt50s$P<-ifelse(lt50s$P=="I",1,ifelse(lt50s$P=="II",2,NA))
lt50s<-filter(lt50s,run_ID!="2")
plot1<-ggplot(lt50s,aes(x=lat,y=LT50))+geom_point()+facet_wrap(acc~P)+geom_smooth(se=F,method="lm")+xlab("latitude,outlier_deaths_included")
```
```{r,echo=F}
plot1
```

### Environmental Data Exploration

Latitude is not a great proxy for temperature. I expect the slope of the LT50~site plot to be different then when latitude is on the x axis. There are a lot of different metrics I can use from the site temperature data I have. I chose four to use: mean sst, summer sst (Jun 01 - September 31), upper 25% quartile of summer sst, and the 90th percentile of summer sst. 

```{r,include=F}
setwd('C:/Users/drewv/Documents/UMASS/SSTdata/rawAtlantic/atl')
library(scales)

gbj<-read.csv('gbj.csv',header=T)
gbj$rdate<-as.POSIXct(gbj$DateTimeStamp,tz="","%m/%d/%Y%H:%M")
wh<-read.csv('wh.csv',header=T)
wh$rdate<-as.POSIXct(wh$DateTimeStamp,tz="", "%m/%d/%Y%H:%M")
oy<-read.csv('oy.csv',header=T)
oy$rdate<-as.POSIXct(oy$DateTimeStamp,tz="", "%m/%d/%Y%H:%M")
bf<-read.csv('bf.csv',header=T)
bf$rdate<-as.POSIXct(bf$DateTimeStamp,tz="", "%m/%d/%Y%H:%M")
fb<-read.csv('fb.csv',header=T)
fb$rdate<-as.POSIXct(fb$DateTimeStamp,tz="", "%m/%d/%Y%H:%M")
dm<-read.csv('dm.csv',header=TRUE)
dm$rdate<-as.POSIXct(dm$DateTimeStamp,tz="","%m/%d/%Y")
gcsk<-read.csv('gcsk.csv',header=TRUE)
gcsk$rdate<-as.POSIXct(gcsk$DateTimeStamp,tz="","%m/%d/%Y%H:%M")#grove creek! Closer
atll<-rbind(gcsk,fb,bf,oy,wh,gbj)
```
```{r,include=F}
setwd('C:/Users/drewv/Documents/UMASS/SSTdata/rawPacific/pac')
wp<-read.csv('wp.csv',header=T)
wp$rdate<-as.POSIXct(wp$DateTimeStamp,tz="","%m/%d/%Y%H:%M")#incomplete! 
to3<-read.csv('to3.csv',header=TRUE)
to3$rdate<-as.POSIXct(to3$DateTimeStamp,tz="","%m/%d/%Y%H:%M")##stitched 2014 and 2015 (post Nove. 21 data together)
hm2<-read.csv('hm2.csv',header=T)
hm2$rdate<-as.POSIXct(hm2$DateTimeStamp,tz="", "%m/%d/%Y%H:%M")
hmi<-read.csv('hmi.csv',header=T)
hmi$rdate<-as.POSIXct(hmi$DateTimeStamp,tz="", "%m/%d/%Y%H:%M")#Indian island 2018+2019
nah1516<-read.csv('nahcotta_2015_2016.csv',header=T)#2015 through August, 2016 data after that
nah1516$rdate<-as.POSIXct(nah1516$DateTimeStamp,tz="", "%m/%d/%Y%H:%M")
hmi2<-read.csv('hmi2.csv',header=T)
hmi2$rdate<-as.POSIXct(hmi2$DateTimeStamp,tz="", "%m/%d/%Y%H:%M")#Indian island 2015

pac2<-rbind(to3,hmi2,nah1516)
```
```{r,include=F}
#loading temperature data
library(dplyr)
library(lubridate)
setwd('C:/Users/drewv/Documents/UMASS/SSTdata/rawAtlantic/atl')
gbj<-read.csv('gbj.csv',header=T)
gbj$rdate<-as.POSIXct(gbj$DateTimeStamp,tz="","%m/%d/%Y%H:%M")
gbj$oce<-"a"
wh<-read.csv('wh.csv',header=T)
wh$rdate<-as.POSIXct(wh$DateTimeStamp,tz="", "%m/%d/%Y%H:%M")
wh$oce<-"a"
oy<-read.csv('oy.csv',header=T)
oy$rdate<-as.POSIXct(oy$DateTimeStamp,tz="", "%m/%d/%Y%H:%M")
oy$oce<-"a"
bf<-read.csv('bf.csv',header=T)
bf$rdate<-as.POSIXct(bf$DateTimeStamp,tz="", "%m/%d/%Y%H:%M")
bf$oce<-"a"
fb<-read.csv('fb.csv',header=T)
fb$rdate<-as.POSIXct(fb$DateTimeStamp,tz="", "%m/%d/%Y%H:%M")
fb$oce<-"a"
setwd('C:/Users/drewv/Documents/UMASS/SSTdata/rawPacific/pac')
nah1516<-read.csv('nahcotta_2015_2016.csv',header=T)#2015 through August, 2016 data after that
nah1516$rdate<-as.POSIXct(nah1516$DateTimeStamp,tz="", "%m/%d/%Y%H:%M")
nah1516$oce<-"p"
hmi2<-read.csv('hmi2.csv',header=T)
hmi2$rdate<-as.POSIXct(hmi2$DateTimeStamp,tz="", "%m/%d/%Y%H:%M")#Indian island 2015
hmi2$oce<-"p"
to3<-read.csv('to3.csv',header=TRUE)
to3$rdate<-as.POSIXct(to3$DateTimeStamp,tz="","%m/%d/%Y%H:%M")##stitched 2014 and 2015 (post Nove. 21 data together)
to3$oce<-"p"

s.gbj<-filter(gbj,rdate>"2018-06-01 00:00:00" & rdate< "2018-09-30 00:00:00")
s.wh<-filter(wh,rdate>"2018-06-01 00:00:00" & rdate< "2018-09-30 00:00:00")
s.oy<-filter(oy,rdate>"2018-06-01 00:00:00" & rdate< "2018-09-30 00:00:00")
s.bf<-filter(bf,rdate>"2018-06-01 00:00:00" & rdate< "2018-09-30 00:00:00")
s.fb<-filter(fb,rdate>"2018-06-01 00:00:00" & rdate< "2018-09-30 00:00:00")
s.to3<-filter(to3,rdate>"2018-06-01 00:00:00" & rdate< "2018-09-30 00:00:00")
s.hmi2<-filter(hmi2,rdate>"2018-06-01 00:00:00" & rdate< "2018-09-30 00:00:00")
s.nah1516<-filter(nah1516,rdate>"2018-06-01 00:00:00" & rdate< "2018-09-30 00:00:00")


##Quartiles
q<-data.frame("site" = c("gbj","wh","oy","bf","fb","nah1516","hmi2","to3"),"quantile"=NA,"decile"=NA, "max"=NA)
q[1,2]<-quantile(s.gbj$WTMP,0.75,type=1)
q[2,2]<-quantile(s.wh$WTMP,0.75,type=1)
q[3,2]<-quantile(s.oy$WTMP,0.75,type=1)
q[4,2]<-quantile(s.bf$WTMP,0.75,type=1)
q[5,2]<-quantile(s.fb$WTMP,0.75,type=1)
q[6,2]<-quantile(s.nah1516$WTMP,0.75,type=1)
q[7,2]<-quantile(s.hmi2$WTMP,0.75,type=1)
q[8,2]<-quantile(s.to3$WTMP,0.75,type=1)

q[1,3]<-quantile(s.gbj$WTMP,0.9,type=1)
q[2,3]<-quantile(s.wh$WTMP,0.9,type=1)
q[3,3]<-quantile(s.oy$WTMP,0.9,type=1)
q[4,3]<-quantile(s.bf$WTMP,0.9,type=1)
q[5,3]<-quantile(s.fb$WTMP,0.9,type=1)
q[6,3]<-quantile(s.nah1516$WTMP,0.9,type=1)
q[7,3]<-quantile(s.hmi2$WTMP,0.9,type=1)
q[8,3]<-quantile(s.to3$WTMP,0.9,type=1)

q[1,4]<-s.gbj %>% group_by(WTMP) %>% summarise(Value = max(WTMP))
q[2,4]<-s.wh %>% group_by(WTMP) %>% summarise(Value = max(WTMP))
q[3,4]<-s.oy %>% group_by(WTMP) %>% summarise(Value = max(WTMP))
q[4,4]<-s.bf %>% group_by(WTMP) %>% summarise(Value = max(WTMP))
q[5,4]<-s.fb %>% group_by(WTMP) %>% summarise(Value = max(WTMP))
q[6,4]<-s.nah1516 %>% group_by(WTMP) %>% summarise(Value = max(WTMP))
q[7,4]<-s.hmi2 %>% group_by(WTMP) %>% summarise(Value = max(WTMP))
q[8,4]<-s.to3 %>% group_by(WTMP) %>% summarise(Value = max(WTMP))


#MEAN SST
temp<-rbind(pac2,atll) #temperature data for all

means<-data.frame(with(temp,tapply(WTMP,site,mean)))

temp$means<-ifelse(temp$site=="bf",means[6,1], ifelse(temp$site=="oy",means[7,1],ifelse(temp$site=="wh",means[8,1],ifelse(temp$site=="gbj",means[9,1],ifelse(temp$site=="to3",means[1,1],ifelse(temp$site=="hmi2",means[2,1],ifelse(temp$site=="nah1516",means[3,1],ifelse(temp$site=="GCSK",means[4,1],ifelse(temp$site=="fb",means[5,1],NA))))))))) #stitching temperature data to sites

temp$site_ID<-ifelse(temp$site=="bf","BF", ifelse(temp$site=="oy","OY",ifelse(temp$site=="wh","WH",ifelse(temp$site=="gbj","GB",ifelse(temp$site=="to3","TO",ifelse(temp$site=="hmi2","HM",ifelse(temp$site=="nah1516","WP",ifelse(temp$site=="GCSK","SK",ifelse(temp$site=="fb","FB",NA))))))))) 





lt50s$mean.sst<-ifelse(lt50s$site=="BF",means[6,1], ifelse(lt50s$site=="OY",means[7,1],ifelse(lt50s$site=="WH",means[8,1],ifelse(lt50s$site=="GB",means[9,1],ifelse(lt50s$site=="TO",means[1,1],ifelse(lt50s$site=="HM",means[2,1],ifelse(lt50s$site=="WP",means[3,1],ifelse(lt50s$site=="SK",means[4,1],ifelse(lt50s$site=="FB",means[5,1],NA))))))))) #the same

 


##SUMMER MEAN SST

summer.temp<-filter(temp,rdate>"2018-06-01 00:00:00" & rdate< "2018-09-30 00:00:00")
s.mean<-data.frame(with(summer.temp,tapply(WTMP,site,mean)))

lt50s$s.mean.sst<-ifelse(lt50s$site=="BF",s.mean[6,1], ifelse(lt50s$site=="OY",s.mean[7,1],ifelse(lt50s$site=="WH",s.mean[8,1],ifelse(lt50s$site=="GB",s.mean[9,1],ifelse(lt50s$site=="TO",s.mean[1,1],ifelse(lt50s$site=="HM",s.mean[2,1],ifelse(lt50s$site=="WP",s.mean[3,1],ifelse(lt50s$site=="SK",s.mean[4,1],ifelse(lt50s$site=="FB",s.mean[5,1],NA)))))))))

##UPPER QUARTILE SUMMER MEAN SST


lt50s$q.mean.sst<-ifelse(lt50s$site=="BF",q[4,2], ifelse(lt50s$site=="OY",q[3,2],ifelse(lt50s$site=="WH",q[2,2],ifelse(lt50s$site=="GB",q[1,2],ifelse(lt50s$site=="TO",q[8,2],ifelse(lt50s$site=="HM",q[7,2],ifelse(lt50s$site=="WP",q[6,2],ifelse(lt50s$site=="FB",q[5,2],NA))))))))

##UPPER 90th PERCENTILE SUMMER MEAN SST


lt50s$t.mean.sst<-ifelse(lt50s$site=="BF",q[4,3], ifelse(lt50s$site=="OY",q[3,3],ifelse(lt50s$site=="WH",q[2,3],ifelse(lt50s$site=="GB",q[1,3],ifelse(lt50s$site=="TO",q[8,3],ifelse(lt50s$site=="HM",q[7,3],ifelse(lt50s$site=="WP",q[6,3],ifelse(lt50s$site=="FB",q[5,3],NA))))))))

lt50s$oce<-ifelse(lt50s$site=="GB","a",ifelse(lt50s$site=="WH","a",ifelse(lt50s$site=="OY","a",ifelse(lt50s$site=="BF","a",ifelse(lt50s$site=="FB","a",ifelse(lt50s$site=="SK","a",ifelse(lt50s$site=="TO","p",ifelse(lt50s$site=="HM","p",NA)))))))) #assigning ocean

##max
lt50s$max<-ifelse(lt50s$site=="BF",q[4,4], ifelse(lt50s$site=="OY",q[3,4],ifelse(lt50s$site=="WH",q[2,4],ifelse(lt50s$site=="GB",q[1,4],ifelse(lt50s$site=="TO",q[8,4],ifelse(lt50s$site=="HM",q[7,4],ifelse(lt50s$site=="WP",q[6,4],ifelse(lt50s$site=="FB",q[5,4],NA))))))))



```
## Comparison of LT50 by four target environmental metrics. 

```{r,echo=F}

summary(lt50s)

ggplot(lt50s,aes(x=mean.sst,y=LT50, group=interaction(lat),color=site))+geom_point()+facet_wrap(acc~P)+geom_smooth(se=F,method="lm",aes(group=1))+xlab("meansst")

ggplot(lt50s,aes(x=s.mean.sst,y=LT50, group=interaction(lat),color=site))+geom_point()+facet_wrap(acc~P)+geom_smooth(se=F,method="lm",aes(group=1))+xlab("summer_meansst")

ggplot(lt50s,aes(x=q.mean.sst,y=LT50, group=interaction(site),color=site))+geom_point()+facet_wrap(acc~P)+geom_smooth(se=F,method="lm",aes(group=1))+xlab("quart.mean.sst")

ggplot(lt50s,aes(x=t.mean.sst,y=LT50, group=interaction(site),color=site))+geom_point()+facet_wrap(acc~P)+geom_smooth(se=F,method="lm",aes(group=1))+xlab("quart.mean.sst")

```
```{r,echo=F,warning=F}
atll2<-rbind(fb,bf,oy,wh,gbj)
pac3<-rbind(to3,hmi2)
all<-rbind(atll2,pac3)
all$date<-as.Date(all$rdate,format="%Y-%m-%d %H:%M:%OS")

ggplot(data=all,aes(x=date,y=WTMP,color=site,group=site))+scale_color_viridis_d(name="Site",labels=c("Great Bay, NH","Woods Hole, MA","Oyster, VA","Beaufort, NC","Folly Beach, SC","Humboldt, CA","Tomales,CA"))+theme_classic()+geom_vline(xintercept = 152)+geom_vline(xintercept = 273)+ylab("SST (째C)")+xlab("")+scale_fill_discrete(name="Site",labels=c("Folly Beach, SC", "Beaufort, NC","Oyster, OY","Woods Hole, MA","Great Bay, NH","Tomales,CA","Humboldt, CA","Willapa, WA"))+scale_x_date(date_labels = "%b")+ 
  theme(axis.text.y=element_text(size=12),axis.title.x=element_text(size=22,vjust=-.1),axis.text.x=element_text(size=12),
  panel.grid.major=element_blank(),panel.grid.minor=element_blank(),
  axis.title.y=element_text(size=22),legend.text=element_text(size=14),legend.title=element_text(size=22))+
  geom_smooth(aes(group=site),se=F,size=2)+guides(color=guide_legend(override.aes=list(fill=NA)))
#stat_summary(geom="ribbon",fun.data=mean_cl_normal,alpha=0.3)+stat_summary(geom="line",fun.y=mean)




##which dates have a mean above 24C?
library(plyr)
library(tidyr)
library(xts)

row.has.na <- apply(bf, 1, function(x){any(is.na(x))})
bf <- bf[!row.has.na,]
dfX <- xts(bf$WTMP, as.Date(bf$rdate))
bf_24<-apply.daily(dfX, mean)

row.has.na <- apply(fb, 1, function(x){any(is.na(x))})
fb <- fb[!row.has.na,]
dfX <- xts(fb$WTMP, as.Date(fb$rdate))
fb_24<-apply.daily(dfX, mean)

row.has.na <- apply(oy, 1, function(x){any(is.na(x))})
oy <- oy[!row.has.na,]
dfX <- xts(oy$WTMP, as.Date(oy$rdate))
oy_24<-as.data.frame(apply.daily(dfX, mean))

```

## Competing environmental data/ $CT_{max}$ models

Because we don't want to overparameterize our models and we're considering booting protocol 2 to the supplement, we will perform two rounds of analysis, one on only Protocol 1 over two acclimations, and one set on only acclimation 20 over one protocol (1).

# Comparing lt50s among acclimations (protocol 1)

```{r,echo=F,warning=F}
c<-cor(lt50s[,c(8:12)])
chart.Correlation(c)
prot1<-filter(lt50s,P=="1")
mods<-list(
"null"=lm(LT50~1,prot1),
"lat"=lm(LT50~lat,prot1),#latitude
"mean"=lm(LT50~mean.sst,prot1),#mean yearly sst
"s.mean"=lm(LT50~s.mean.sst,prot1),#mean summer sst (06/01 - 09/01)
"q.mean"=lm(LT50~q.mean.sst,prot1),#upper 25% quartile of summer mean sst
"t.mean"=lm(LT50~t.mean.sst,prot1),#upper 10th percentile of summer mean sst
"acc"=lm(LT50~acc,prot1),
"lat.acc"=lm(LT50~lat+acc,prot1),
"mean.acc"=lm(LT50~mean.sst+acc,prot1),
"s.mean.acc"=lm(LT50~s.mean.sst,prot1),
"q.mean.acc"=lm(LT50~q.mean.sst+acc,prot1),
"t.mean.acc"=lm(LT50~t.mean.sst+acc,prot1),
"lat*acc"=lm(LT50~lat*acc,prot1),
"mean*acc"=lm(LT50~mean.sst*acc,prot1),
"s.mean*acc"=lm(LT50~s.mean.sst*acc,prot1),
"q.mean*acc"=lm(LT50~q.mean.sst*acc,prot1),
"t.mean*acc"=lm(LT50~t.mean.sst*acc,prot1),
"olat"=lm(LT50~lat+oce,prot1),#latitude
"omean"=lm(LT50~mean.sst+oce,prot1),#mean yearly sst
"os.mean"=lm(LT50~s.mean.sst+oce,prot1),#mean summer sst (06/01 - 09/01)
"oq.mean"=lm(LT50~q.mean.sst+oce,prot1),#upper 25% quartile of summer mean sst
"ot.mean"=lm(LT50~t.mean.sst+oce,prot1),#upper 10th percentile of summer mean sst
"oacc"=lm(LT50~acc+oce,prot1),
"olat.acc"=lm(LT50~lat+acc+oce,prot1),
"omean.acc"=lm(LT50~mean.sst+acc+oce,prot1),
"os.mean.acc"=lm(LT50~s.mean.sst+acc+oce,prot1),
"oq.mean.acc"=lm(LT50~q.mean.sst+acc+oce,prot1),
"ot.mean.acc"=lm(LT50~t.mean.sst+acc+oce,prot1),
"olat*acc"=lm(LT50~lat*acc*oce,prot1),
"omean*acc"=lm(LT50~mean.sst*acc*oce,prot1),
"os.mean*acc"=lm(LT50~s.mean.sst*acc*oce,prot1),
"oq.mean*acc"=lm(LT50~q.mean.sst*acc*oce,prot1),
"ot.mean*acc"=lm(LT50~t.mean.sst*acc*oce,prot1),
"max"=lm(LT50~max,prot1),
"maxacc"=lm(LT50~max+acc,prot1),
"max*acc"=lm(LT50~max*acc,prot1),
"maxoacc"=lm(LT50~max+acc+oce,prot1),
"maxoacc*"=lm(LT50~max*acc*oce,prot1))

summary(glm(LT50~acc,prot1,family="gaussian"))
ggplot(prot1,aes(x=acc,y=LT50))+geom_boxplot()

aictab(mods)
mod1=(mods$'max*acc')
mod3=lm(LT50~max*acc,prot1)
summary(lm(LT50~max*acc,prot1))
summary(mod1)
summary(mod3)

r.squaredGLMM(mod1)
#plot(mod1)
E1<-resid(mod1,type="pearson")
N<-nrow(prot1)
p<-length(coef(mod1))
sum(E1^2) / (N-p)
#no overdispersion

make_model<-function(prot1) {glm(LT50 ~ t.mean.sst,prot1,family="gaussian")}
hist(resid(mod1))
prot1$acc<-as.factor(prot1$acc)

#make models for both acclimation temperatures
models1<-dlply(prot1,"acc",.fun=make_model)
predvals.acc<-ldply(models1,.fun=predictvals,xvar="t.mean.sst",yvar="LT50")

fig5<-ggplot(prot1,aes(x=max,y=LT50,color=acc,shape=oce,fill=acc))+geom_point(size=3)+theme_classic()+scale_color_manual(labels=c("20째C","24째C"),name="Acclimation",values=c("blue","red"))+theme_classic()+xlab("Habitat Temperature (SST 째C")+scale_fill_manual(labels=c("20째C","24째C"),name="Acclimation",values=c("blue","red"))+scale_shape_manual(labels=c("Atlantic","Pacific"),name="Ocean",values=c(21,24))+
  theme(text=element_text(family="arial",size=22))+labs(y=expression(paste("LT"[50],  " (째C)")), x=expression(paste("T"[hab], " (SST 째C)")))+scale_x_continuous(breaks=c(20,22,24,26,28,30,32,34),limits=c(20,34))+scale_y_continuous(breaks=c(35,36,37,38,39,40,41),limits=c(35,41))+  
  geom_segment(aes(x = 21.8, xend = 33.56934, y = 37.97841       + 0.04861      *21.8	, yend = 37.97841       + 0.04861       *33.56934	),size=1.5,color='blue')+
geom_segment(aes(x = 21.8, xend = 33.56934, y = (37.97841+4.15862)   + (0.04861-0.18156 )  *21.8	, yend = (37.97841+4.15862)   + (0.04861-0.18156 )   *33.56934	),size=1.5,color='red')
  


acc20a<-filter(lt50s,P=="1")
acc20a<-filter(acc20a,acc=="20")
acc24a<-filter(lt50s,P=="2")
acc24a<-filter(acc24a,acc=="24")
mean(acc20a$LT50)
mean(acc24a$LT50)
sqrt(var(acc20a$LT50))
sqrt(var(acc24a$LT50))
```


## Comparing protocols 1 and 2 (acclimation 20) 

```{r,echo=T}
acc20<-filter(lt50s,acc=="20")
mods2<-list(
"null"=lm(LT50~1,acc20),
"lat"=lm(LT50~lat,acc20),#latitude
"mean"=lm(LT50~mean.sst,acc20),#mean yearly sst
"s.mean"=lm(LT50~s.mean.sst,acc20),#mean summer sst (06/01 - 09/01)
"q.mean"=lm(LT50~q.mean.sst,acc20),#upper 25% quartile of summer mean sst
"t.mean"=lm(LT50~t.mean.sst,acc20),#upper 10th percentile of summer mean sst
"P"=lm(LT50~P,acc20),
"lat.P"=lm(LT50~lat+P,acc20),
"mean.P"=lm(LT50~mean.sst+P,acc20),
"s.mean.P"=lm(LT50~s.mean.sst,acc20),
"q.mean.P"=lm(LT50~q.mean.sst+P,acc20),
"t.mean.P"=lm(LT50~t.mean.sst+P,acc20),
"lat*P"=lm(LT50~lat*P,acc20),
"mean*P"=lm(LT50~mean.sst*P,acc20),
"s.mean*P"=lm(LT50~s.mean.sst*P,acc20),
"q.mean*P"=lm(LT50~q.mean.sst*P,acc20),
"t.mean*P"=lm(LT50~t.mean.sst*P,acc20),
"olat"=lm(LT50~lat+oce,acc20),#latitude
"omean"=lm(LT50~mean.sst+oce,acc20),#mean yearly sst
"os.mean"=lm(LT50~s.mean.sst+oce,acc20),#mean summer sst (06/01 - 09/01)
"oq.mean"=lm(LT50~q.mean.sst+oce,acc20),#upper 25% quartile of summer mean sst
"ot.mean"=lm(LT50~t.mean.sst+oce,acc20),#upper 10th percentile of summer mean sst

"olat.P"=lm(LT50~lat+P+oce,acc20),
"omean.P"=lm(LT50~mean.sst+P+oce,acc20),
"os.mean.P"=lm(LT50~s.mean.sst+P+oce,acc20),
"oq.mean.P"=lm(LT50~q.mean.sst+P+oce,acc20),
"ot.mean.P"=lm(LT50~t.mean.sst+P+oce,acc20),
"olat*P"=lm(LT50~lat*P*oce,acc20),
"omean*P"=lm(LT50~mean.sst*P*oce,acc20),
"os.mean*P"=lm(LT50~s.mean.sst*P*oce,acc20),
"oq.mean*P"=lm(LT50~q.mean.sst*P*oce,acc20),
"ot.mean*P"=lm(LT50~t.mean.sst*P*oce,acc20),
"max"=lm(LT50~max,acc20),
"maxacc"=lm(LT50~max+acc,acc20),
"max*acc"=lm(LT50~max*acc,acc20),
"maxoacc"=lm(LT50~max+acc+oce,acc20),
"maxoacc*"=lm(LT50~max*acc*oce,acc20))

aictab(mods2)
mod2<-(mods2$`q.mean.P`)
summary(mod2)
hist(resid(mod2))
#plot(mod2)

model<-glm(LT50 ~q.mean.sst+t.mean.sst+s.mean.sst+mean.sst+lat,data=acc20,family="gaussian")

1/(car::vif(model))

a<-glm(LT50~P,acc20, family="gaussian")
b<-glm(LT50~q.mean.sst+P,acc20, family="gaussian")
c<-glm(LT50~t.mean.sst+P,acc20, family="gaussian")
summary(model.avg(a,b,c))

E1<-resid(mod2,type="pearson")
N<-nrow(acc20)
p<-length(coef(mod2))
sum(E1^2) / (N-p)
#no overdispersion

make_model_acc<-function(acc20) {lm(LT50 ~ q.mean.sst,acc20)}
acc20$P<-as.factor(acc20$P)

#make models for both acclimation temperatures
models2<-dlply(acc20,"P",.fun=make_model_acc)
predvals.P<-ldply(models2,.fun=predictvals,xvar="q.mean.sst",yvar="LT50")

acc20$P<-as.factor(acc20$P)

fig6<-ggplot(acc20,aes(x=q.mean.sst,y=LT50,color=P,fill=P,shape=oce))+geom_point(size=3)+xlab("Mean SST 째C (Upper quartile summer SST)")+scale_color_manual(labels=c("1","2"),name="Protocol",values=c("purple","darkgreen"))+theme_classic()+scale_shape_manual(labels=c("Atlantic","Pacific"),name="Ocean",values=c(21,24))+scale_fill_manual(labels=c("1","2"),name="Protocol",values=c("purple","darkgreen"))

fig6a<-fig6+theme(text=element_text(family="sanserif",size=22))+labs(y=expression(paste("LT"[50],  " (째C)")),x=expression(paste("T"[hab],"(SST 째C)")))+scale_x_continuous(breaks=c(18,20,22,24,26,28,30,32),limits=c(18,32))+scale_y_continuous(breaks=c(37,38,39,40,41),limits=c(37,41))+guides(color=guide_legend(order=1),fill=guide_legend(order=1),shape=guide_legend(order=0))

fig6b<-ggplot(acc20,aes(x=P,y=LT50,fill=P))+geom_boxplot()+theme_classic()+scale_fill_manual(labels=c("1","2"),name="Protocol",values=c("purple","darkgreen"))+theme(text=element_text(family="sanserif",size=22))+labs(y=expression(paste("LT"[50],  " (째C)")),x="Protocol")+scale_y_continuous(breaks=c(37,38,39,40,41),limits=c(37,41))+theme(legend.position = "none")

prot1a<-filter(lt50s,P=="1")
prot1a<-filter(prot1a,acc=="20")
prot2a<-filter(lt50s,P=="2")
prot2a<-filter(prot2a,acc=="20")
mean(prot1a$LT50)
mean(prot2a$LT50)
sqrt(var(prot1a$LT50))
sqrt(var(prot2a$LT50))

library(ggpubr)
ggarrange(fig6a,fig6b,ncol=2,nrow=1,widths=c(2,1))
```

Table of run statistics based on runs used (will have to edit if I include run 3)

```{r}
setwd("C:/Users/drewv/Documents/UMASS/data")
run.stats<-na.omit(read.csv('run.stats.csv'))
data.table(run.stats)



acc20 %>% group_by(P) %>% summarise_each(funs(mean, sd))
```
```{r,include=F}

#sumstats<-as.data.frame(summary(data))

#write_tsv(sumstats,path="sumstats.txt")
```
## Warming Tolerance

Deutsch et al. (2008 PNAS) discuss "Warming Tolerance," which they define as WT = $CT_{max}$ -$T_{hab}$, or the difference between thermal tolerance and temperature of the habitat. $T_{hab}$ is usually measured as the mean temperature. The smaller the warming tolerance is, they more likely that population is to be vulnerable to climate change. I will calculate a mean WT, an upper 90th percentile wt, and Wt with habitat maximum.
```{r}
lt50s$wt<-lt50s$LT50-lt50s$mean.sst
lt50s$wt.t<-lt50s$LT50-lt50s$t.mean.sst
prot1$wt.t<-prot1$LT50-prot1$t.mean.sst
prot1$wt.max<-prot1$LT50-prot1$max
prot1$lat<-as.numeric(prot1$lat)
prot1$acc<-as.factor(prot1$acc)
ggplot(lt50s,aes(x=lat,y=wt.t,color=oce,group=lat))+geom_boxplot()+ylab("Warming tolerance, Mean SST")
prot1 %>% group_by(acc) %>% summarise_each(funs(mean, sd))


lt50s%>%
  group_by(c(site,acc))%>%
  summarise_each(funs(mean, sd),wt.t)


#90th percentile, ocean data included but not in model

ggplot(prot1,aes(x=t.mean.sst,y=wt.t,fill=acc,group=lat,shape=oce,color=acc))+theme_classic()+theme(text=element_text(family="sanserif",size=22))+labs(y="Warming Tolerance",x="Latitude")+scale_fill_manual(labels=c("20째C","24째C"),name="Acclimation", values=c('blue','red'))+scale_color_manual(labels=c("20째C","24째C"),name="Acclimation",values=c('blue','red'))+scale_x_continuous(name=expression(paste("T"[hab], " (SST 째C)")),limits=c(18,32),breaks=c(18,20,22,24,26,28,30,32))+scale_shape_manual(name="Ocean",labels = c("Atlantic","Pacific"),values=c(21,24))+geom_segment(aes(x = 19.78000, xend = 30, y = 39.88198      + -1.02166 *19.78000, yend = 39.88198      + -1.02166 *30.07281	),size=1.5,color='blue')+geom_point(size=3)


prot1atl<-filter(prot1,oce!="p")



##Warming tolerance using Max as Thab
warm.max<-list(
"a.1"<-glm(wt.max~lat,prot1,family="gaussian"),
"b.1"<-glm(wt.max~mean.sst,prot1,family="gaussian"),
"c.1"<-glm(wt.max~s.mean.sst,prot1,family="gaussian"),
"d.1"<-glm(wt.max~q.mean.sst,prot1,family="gaussian"),
"e.1"<-glm(wt.max~t.mean.sst,prot1,family="gaussian"),
"f.1"<-glm(wt.max~lat+acc,prot1,family="gaussian"),
"g.1"<-glm(wt.max~mean.sst+acc,prot1,family="gaussian"),
"h.1"<-glm(wt.max~s.mean.sst+acc,prot1,family="gaussian"),
"i.1"<-glm(wt.max~q.mean.sst+acc,prot1,family="gaussian"),
"j.1"<-glm(wt.max~t.mean.sst+acc,prot1,family="gaussian"),
"k.1"<-glm(wt.max~acc,prot1,family="gaussian"),
"l.1"<-glm(wt.max~lat*acc,prot1,family="gaussian"),
"m.1"<-glm(wt.max~mean.sst*acc,prot1,family="gaussian"),
"n.1"<-glm(wt.max~s.mean.sst*acc,prot1,family="gaussian"),
"o.1"<-glm(wt.max~q.mean.sst*acc,prot1,family="gaussian"),
"p.1"<-glm(wt.max~t.mean.sst*acc,prot1,family="gaussian"),
"a.2"<-glm(wt.max~lat+oce,prot1,family="gaussian"),
"b.2"<-glm(wt.max~mean.sst+oce,prot1,family="gaussian"),
"c.2"<-glm(wt.max~s.mean.sst+oce,prot1,family="gaussian"),
"d.2"<-glm(wt.max~q.mean.sst+oce,prot1,family="gaussian"),
"e.2"<-glm(wt.max~t.mean.sst+oce,prot1,family="gaussian"),
"f.2"<-glm(wt.max~lat+acc+oce,prot1,family="gaussian"),
"g.2"<-glm(wt.max~mean.sst+acc+oce,prot1,family="gaussian"),
"h.2"<-glm(wt.max~s.mean.sst+acc+oce,prot1,family="gaussian"),
"i.2"<-glm(wt.max~q.mean.sst+acc+oce,prot1,family="gaussian"),
"j.2"<-glm(wt.max~t.mean.sst+acc+oce,prot1,family="gaussian"),
"k.2"<-glm(wt.max~acc+oce,prot1,family="gaussian"),
"l.2"<-glm(wt.max~lat*acc*oce,prot1,family="gaussian"),
"m.2"<-glm(wt.max~mean.sst*acc*oce,prot1,family="gaussian"),
"n.2"<-glm(wt.max~s.mean.sst*acc*oce,prot1,family="gaussian"),
"o.2"<-glm(wt.max~q.mean.sst*acc*oce,prot1,family="gaussian"),
"p.2"<-glm(wt.max~t.mean.sst*acc*oce,prot1,family="gaussian"),
"q.2"<-glm(wt.max~max,prot1,family="gaussian"),
"r.2"<-glm(wt.max~max+acc,prot1,family="gaussian"),
"s.2"<-glm(wt.max~max*acc,prot1,family="gaussian"),
"t.2"<-glm(wt.max~max+acc+oce,prot1,family="gaussian"),
"u.2"<-glm(wt.max~max*acc*oce,prot1,family="gaussian"))

aictab(warm.max)
summary(s.2)
summary(lm(wt.max~max*acc,prot1))

ggplot(prot1,aes(x=max,y=wt.max,fill=acc,group=lat,shape=oce,color=acc))+theme_classic()+theme(text=element_text(family="sanserif",size=22))+labs(y="Warming Tolerance",x="Latitude")+scale_fill_manual(labels=c("20째C","24째C"),name="Acclimation", values=c('blue','red'))+scale_color_manual(labels=c("20째C","24째C"),name="Acclimation",values=c('blue','red'))+scale_shape_manual(name="Ocean",labels = c("Atlantic","Pacific"),values=c(21,24))+geom_segment(aes(x = 21.8, xend = 33.56934, y = 37.97841 -(0.95139 *21.8), yend = 37.97841 -(0.95139 *33.56934)	),size=1.5,color='blue')+geom_point(size=3)+scale_x_continuous(name=expression(paste("T"[hab], " (SST 째C)")),limits=c(20,34),breaks=c(20,22,24,26,28,30,32,34))+
  geom_segment(aes(x = 21.8, xend = 33.56934, y = (37.97841+4.15862 ) +((-0.95139-0.18156) *21.8), yend = (37.97841+4.15862 ) +((-0.95139-0.18156) *33.56934)	),size=1.5,color='red')+
  scale_y_continuous(name="Warming Tolerance (째C)",limits=c(0,20),breaks=c(0,4,8,12,16,20)) 


```

