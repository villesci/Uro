---
title: "Thermal and warming tolerance analysis"
author: "Andrew Villeneuve"
date: "08/07/2020"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(tinytex.verbose = TRUE)
library(knitr)
library(rmarkdown)
library(lme4)
library(ggplot2) 
library(MuMIn)
library(raster)
library(dplyr)
library(gdata)
library(logistf)
library(data.table)
library(stringr)
library(PerformanceAnalytics)
library(bsts)
library(brglm)
library(AICcmodavg)
library(plyr)
library(extrafont)
library(here)
library(magrittr)
library(lubridate)
library(tidyr)
library(xts)

loadfonts(device = "win")
 

lt50<- function(model,value) {
    function(x) {
      predict(model,data.frame(t5=x),type="response")-value}}

# Given a model, predict values of yvar from xvar
# This supports one predictor and one predicted variable.
# xrange: If NULL, determine the x range from the model object. If a vector with
#   two numbers, use those as the min and max of the prediction range.
# samples: Number of samples across the x range.
# ...: Further arguments to be passed to predict()
predictvals <- function(model, xvar, yvar, xrange=NULL, samples=100, ...) {

  # If xrange isn't passed in, determine xrange from the models.
  # Different ways of extracting the x range, depending on model type
  if (is.null(xrange)) {
    if (any(class(model) %in% c("lm", "glm")))
      xrange <- range(model$model[[xvar]])
    else if (any(class(model) %in% "loess"))
      xrange <- range(model$x)
  }

  newdata <- data.frame(x = seq(xrange[1], xrange[2], length.out = samples))
  names(newdata) <- xvar
  newdata[[yvar]] <- predict(model, newdata = newdata, ...)
  newdata
}

summarySE <- function(data=NULL, measurevar, groupvars=NULL, na.rm=FALSE,
                      conf.interval=.95, .drop=TRUE) {
    library(plyr)

    # New version of length which can handle NA's: if na.rm==T, don't count them
    length2 <- function (x, na.rm=FALSE) {
        if (na.rm) sum(!is.na(x))
        else       length(x)
    }

    # This does the summary. For each group's data frame, return a vector with
    # N, mean, and sd
    datac <- ddply(data, groupvars, .drop=.drop,
      .fun = function(xx, col) {
        c(N    = length2(xx[[col]], na.rm=na.rm),
          mean = mean   (xx[[col]], na.rm=na.rm),
          sd   = sd     (xx[[col]], na.rm=na.rm)
        )
      },
      measurevar
    )

    # Rename the "mean" column    
    datac <- rename(datac, c("mean" = measurevar))

    datac$se <- datac$sd / sqrt(datac$N)  # Calculate standard error of the mean

    # Confidence interval multiplier for standard error
    # Calculate t-statistic for confidence interval: 
    # e.g., if conf.interval is .95, use .975 (above/below), and use df=N-1
    ciMult <- qt(conf.interval/2 + .5, datac$N-1)
    datac$ci <- datac$se * ciMult

    return(datac)
}
here::here()
data<-read.csv(here::here("data/master.copy(9.17.19).csv"))
runs<-read.csv(here::here("data/run.count.csv"))
#setwd("C:/Users/drewv/Documents/UMASS/data")
#data<-read.csv("master.copy(9.17.19).csv")
#runs<-read.csv("run.count.csv")
data$acc<-as.factor(data$acc)
data$survival<-as.character(data$survival)

#correcting incorrectly transcribed measurements
which(data$weight>0.007)
data[253,7]<-.0017
which(data$t1>60)
data[474,9]<-24.8
data[504,9]<-24.8
data[534,9]<-24.8
which(data$t2<10)
data[962,10]<-25.5
data[992,10]<-25.5
data[1022,10]<-25.5
which(data$t5<10)
data[962,13]<-29.3
```
### The Study
The purpose of the $CT_{max}$ study was to evaluate the thermal tolerances (or $CT_{max}$) of *Urosalpinx cinerea* from a range of populations across latitude in both the Atlantic and Pacific Oceans. Further, these populations of snails were incubated, hatched, acclimated at two different temperatures (20° and 24° C), to test for plasticity in thermal tolerances across population. We expect populations from northern "cold" sites to show lower $CT_{max}$ then populations from southern "warm" sites.

Egg cases were reared in aerated tanks at either 20C or 24C until hatching. After hatching, snails were held and fed oyster spat for 8-16 days at their acclimation temperature before they entered the heat bar experiment. 

The heatbar was controlled on one end by a PID controller to control heat ramping, while a cooling water bath was hooked up to the other end. For the published manuscript, we include data from one protocol of heatbar ramping. However, we did use one other ramping protocol in an attempt to create finer divisions between heatbar positions (see the section below addressing alternate protocols). 

The heatbar was designed to test three populations at once, with 30 snails from each population. Prior to the start of the heat bar experiment, each snail was weighed as a proxy for age as well as size to include in models. We did this to ensure that size and age did not affect our results. An individual, weighed snail was placed in a 5ml tube of seawater from the proper acclimation (20 or 24C) temperature. Nitex mesh of 200microns was secured with a plastic collar so that the mesh forms a plug below the waterline. This allows for free exchange of air. Once all snails were placed in tubes and their ID recorded so we could match survivorship with weight, population, and specific snail, we began the heatbar trial. See the separate r file heatbar_ramping.r for the specifics of how the heatbar was ramped over 5 hours. After hours, tubes were immersed in either 20C or 24C water as appropriate overnight to allow for recovery. The next day, we sorted live and dead snails by looking for foot attachment on the sides of each tube or by poking the foot with a probe if unclear. 

There were two main steps in analyzing thermal tolerance data. 1) We used binomial models to extract the LT50 of each run (see metadata for explanation). Essentially, for every 30 snails from a single population and acclimation temp, we got a single LT50 value. 2) We constructed models to test the relationship between population and acclimation LT50s across different environmental parameters. Through model selection, we were able to select which environmental predictor best explained patterns of LT50 with environment. 

## Protocol

We created one other heat bar ramping protocol during the experimental phase of research. This was formulated to decrease the temperature difference between heatbar positions and potentially get a more precise LT50 value. Unfortunately, we did not have enough juvenile snails to achieve the sample sizes necessary to enable a proper analysis. The heatbar protocol for this other ramp is shown in the heatbar_ramping.r file, but the analysis of the LT50 data is not shown in this Rmarkdown. Future research into protocol differences should be undertaken, as initial results were promising.  

## Metadata

# What is a run?
A "run" is a sequence of 15-30 snails for a given population and acclimation. Each "run" has no column replicates, so if we did not randomly distribute them by row they would be in a single row. A heatbar run is when up to three different "runs" are run simultaneously. 

# Date
The date is the date of the heatbar "run" (as defined below) for a given population and acclimation. 

# Site
Two-letter abbreviation of sites used. Correspond to the following sites:

```{r}
site.meta<-data.table(siteID = c("GB","WH","OY","BF","FB","HM","TO"),site=c("Great Bay","Woods Hole","Oyster","Beaufort","Folly Beach","Humboldt","TO"),state=c("NH","MA","VA","NC","SC","CA","CA"))
site.meta
```

# Acc
Acc is the acclimation temperature. Each population was reared in either 20°C or 24°C seawater (30PSU). We are interested in knowing if acclimation temperature will expose signs of plasticity in thermal tolerance for each population.

# col
Col is the column in which the individual from that particular run/population/acc was placed. For both protocols 1 and 2, all snails were placed in positions 9-38. No snails were in 1-8 due to the low temperature in these positions closeest to the cooling element. 

# rand
a random number generated to assign column positions to snails

# row
The row (A,B,C) randomly assigned for each run's snail

# weight
The dry weight of each snail before being used in the heatbar experiment.Snails were dabbed dry to remove excess water on and inside the shell.

# T0-T5
These are the timepoints at which temperatures were taken. Each has a step in between of 1 hr. The table below shows the ramping rate on the PID controller (setpoints) in relation to time and the timepoints at which temperature was measured. 

# Survival
Survival of the snails was established between 15-18 hrs after the experiment end. Tubes of the snails were floated in their acclimation temperature seawater immediately after the experiment for the 15-18 hours. Afterwards, each snail was examined under a microscope and prodded with a probe. If the foot retracted, they were scored as "alive" (or 1), and if not, they were scored as "dead" (or 0). In at least one run, some snails were discovered to have been drilled (cannibalized) before the experiment. These were classed as "d" for drilled, and were not included in analysis. Future runs had each snail examined for signs of drilling before experiment ran. In the run with drilled snails, I counted snails that had tissue in the shell but clearly dead as "0"s.

# Protocol
Three total protocols were tested in the experiment, with only one included in data analysis. Most runs were completed under protocol 1, wherein temperature was ramped from 25C to 60C and ice added every hour for the first two hours of the five hour experiment. Protocol 2 had the same ramping rate but had no ice added. Protocol 3 had no ice added and had a lower end setpoint of 50C. P1 had a very wide range in temperature, and so we developed P2 to lessen the spread between temperature positions. P3 was developed for the same reason, but the projected CTmax point was too close to the heating element (not enough "0"s for the model).


```{r,include=F}
ramp<-data.table(time_hrs  = c("0","0.5","1","1.5","2","2.5","3","3.5","4","4.5","5"),timepoint=c("T0","","T1","","T2","","T3","","T4","","T5"),setpoint_P1P2=c("20/24","25","30","35","40","45","50","55","60","60","60"),P1_ice=c("yes","no","yes","no","yes","no","no","no","no","no","no"),P2P3_ice=c("no","no","no","no","no","no","no","no","no","no","no"),setpoint_P3=c("20/24","25","28","31","34","38","42","46","50","50","50"))
```
```{r,include=T}
ramp
```

In the first heatbar run, I included cannibalized snails in the heatbar. I was able to differentiate at the end of the heatbar run which snails had died because of cannibalisim by looking for drill holes on the shells. These snails were denoted with a "d". Snails which had no drill holes and still had the body of the snail within the shell were counted as mortalities in the heatbar. 

I also need to get rid of protocol 2 runs, as the ramping rate was too dissimilar to protocol 1. 

Finally, not every column in the heatbar was used, and so these are blank rows we can get rid of.

```{r,warning=F,echo=F}
data<-filter(data, survival != "d") #get rid of "drilled" uros 
data<-filter(data, protocol != 3) #get rid of protocol 2
data<-filter(data,run_count>13)
data<-filter(data, weight != "NA")

data = filter(data,site!="BFII")
data$site[data$site=="FB2"] <- "FB"
data$site[data$site=="OYII"] <- "OY"
data$site[data$site=="TO2"] <- "TO"
data = filter(data,site!="FB2")
data = filter(data,site!="OYII")
data = filter(data,site!="TO2")

data$site<-factor(data$site)
data$survival<-as.numeric(data$survival)
#data$survival<-factor(data$survival)


table(data$survival, data$site, data$acc)
tapply(data$survival, data$site, length)
pop<-tapply(data$survival, list(data$site,data$acc), length)
```
Data is now fully cleaned! Time to do some exploration.How well did we sample all the sites across acclimation temperatures?

```{r,include=T}
ggplot(data,aes(x=site, fill=acc))+geom_bar(position="dodge")+facet_wrap(protocol~.)
```
Because coverage was not complete across sites for protocol 2, and ramping rate was not a primary research objective, we did not include protocol 2 data in our analysis. We present the data above for transparency. 

## Visualization of all data

Let's visualize survival data in the heatbar across all sites and protocols. Further, did age of the snails drive survivorship?

```{r, include=F,echo=F}
LT.plot2<-ggplot(data, aes(x=t5,y=survival, color=acc))+
  geom_point(alpha=1/2,size=4,position=position_jitter(width=0,height=.03))+
  theme_bw()+ylab("Survival")+xlab("Temperature in heatbar")+
  theme(axis.text=element_text(size=18),axis.title.x=element_text(size=22,vjust=-.1),
        panel.grid.major=element_blank(),panel.grid.minor=element_blank(),axis.title.y=element_blank())+scale_x_continuous(breaks=c(10,15,20,25,30,35,40,45,50,55,60,65))+scale_y_discrete(breaks=c(0,.5,1))+facet_grid(site~protocol)


```
```{r,include=T,echo=F}
LT.plot2


ggplot(data=data,aes(x=survival,y=weight,group=survival))+geom_boxplot()
summary(aov(survival~weight,data=data))
summary(lm(data=data,weight~acc))
summary(lm(data=data,survival~weight))


```
Survival does not determined by weight, which is our proxy for age. 

### Individual Analysis
Now that I have a workflow, I will start off by producing binary models and extracting $CT{max}$ from each run. This is so I can make a for-loop later to perform this over all runs. 

```{r,include=T,echo=F}
#select a run as an example
run1<-filter(data,run_ID == 1)

#t5 was the final, maximum temperature timepoint in the heatbar for each position
t5<-run1$t5


m1<-brglm(data=run1,survival~t5,family="binomial")
summary(m1)

c0=coef(m1)[1] #model intercept 
c1=coef(m1)[2] #model slope 

p1=plogis(c0+c1*t5)
comb<-data.frame(t5,p1)


p<-ggplot(run1, aes(x=t5,y=survival))+
  geom_point(alpha=1/2,size=4,position=position_jitter(width=0,height=.03))+
  theme_bw()+ylab("Survival")+xlab("Temperature in Heatbar")+
  theme(axis.text=element_text(size=18),axis.title.x=element_text(size=22,vjust=-.1),
        panel.grid.major=element_blank(),panel.grid.minor=element_blank(),axis.title.y =element_blank())+
  scale_x_continuous(breaks=c(10,15,20,25,30,35,40,45,50,55,60,65))+scale_y_discrete(breaks=c(0,.5,1))+geom_line(data=comb,aes(x=t5,y=p1))


#extract the lt50
a1<-uniroot(lt50(m1,0.5),range(run1$t5))$root
a1

p+geom_hline(yintercept=.5)+geom_vline(xintercept=a1)
```

Here we can see how I will analyze each "run." Note that there is an outlier death. 

Below is a loop I used to extact LT50s by run and store in a dataframe. 
```{r,include=F}
##data<-data[-c(652),]
##data<-data[-c(652),]
data<-data[-c(1),] # remove a single false negative
##data<-data[-c(35),]
##data<-data[-c(35),]
#data<-data[-c(524),]
#data<-data[-c(669),]
#data<-data[-c(748),]

#View(which(data$t5<34&data$survival==0))
data<-filter(data,run_ID !=3)
#The above observations and run are problematic and eliminated.
```
```{r,include=T,warning=F}
##forloop

runs<-unique(data$run_ID)
df<-list()
for (i in 1:length(runs)){
  tmp<-filter(data,run_ID==runs[i])
  df[[i]]<-tmp
 
}

modlist<-list()
for (i in 1:length(df)){
  mod<-brglm(data=df[[i]],survival~t5,family="binomial")
  modlist[[i]]<-mod
}
names(modlist)<-runs

LT50<-list()
for(i in 1:length(modlist)){
  lt<-dose.p(modlist[[i]],p=0.5)[1]
  LT50[[i]]<-lt
}

dflt<-data.frame(LT50,ncol=length(LT50))
dflt<-melt(as.data.table(dflt))
dflt<-dflt[-c(31),]#32 if we keep run_ID ==3
dfdlt2<-data.frame(dflt,runs)
lt50s<-subset(dfdlt2,select=-c(variable))
names(lt50s)<-c("LT50","run_ID")
#View(lt50s)

run.count <- read.csv(here::here("data/run.count.csv"))
lt50s<-merge(lt50s,run.count,by="run_ID")
```
I've now extracted all of my $CT{max}$ values. I just need to merge this with other metadata for each run so that I can analyze by environmental predictors 

### Environmental Data Exploration

Latitude is not a great proxy for temperature. I expect the slope of the LT50~site plot to be different then when latitude is on the x axis. There are a lot of different metrics I can use from the site temperature data I have. I chose four to use: mean sst, summer sst (Jun 01 - September 31), upper 25% quartile of summer sst, and the 90th percentile of summer sst. 

```{r,include=F}
#read in site data
gbj<-read.csv(here::here('data/environmental_data/rawAtlantic/atl/gbj.csv'))
gbj$rdate<-as.POSIXct(gbj$DateTimeStamp,tz="","%m/%d/%Y%H:%M")
gbj$oce<-"a"
wh<-read.csv(here::here('data/environmental_data/rawAtlantic/atl/wh.csv'))
wh$rdate<-as.POSIXct(wh$DateTimeStamp,tz="", "%m/%d/%Y%H:%M")
wh$oce<-"a"
oy<-read.csv(here::here('data/environmental_data/rawAtlantic/atl/oy.csv'))
oy$rdate<-as.POSIXct(oy$DateTimeStamp,tz="", "%m/%d/%Y%H:%M")
oy$oce<-"a"
bf<-read.csv(here::here('data/environmental_data/rawAtlantic/atl/bf.csv'))
bf$rdate<-as.POSIXct(bf$DateTimeStamp,tz="", "%m/%d/%Y%H:%M")
bf$oce<-"a"
fb<-read.csv(here::here('data/environmental_data/rawAtlantic/atl/fb.csv'))
fb$rdate<-as.POSIXct(fb$DateTimeStamp,tz="", "%m/%d/%Y%H:%M")
fb$oce<-"a"
atll<-rbind(fb,bf,oy,wh,gbj)

to3<-read.csv(here::here('data/environmental_data/rawPacific/pac/to3.csv'))
to3$rdate<-as.POSIXct(to3$DateTimeStamp,tz="","%m/%d/%Y%H:%M")
to3$oce<-"p"
hmi2<-read.csv(here::here('data/environmental_data/rawPacific/pac/hmi2.csv'))
hmi2$rdate<-as.POSIXct(hmi2$DateTimeStamp,tz="", "%m/%d/%Y%H:%M")
hmi2$oce<-"p"

pac2<-rbind(to3,hmi2)

#create data frames for each
s.gbj<-filter(gbj,rdate>"2018-06-01 00:00:00" & rdate< "2018-09-30 00:00:00")
s.wh<-filter(wh,rdate>"2018-06-01 00:00:00" & rdate< "2018-09-30 00:00:00")
s.oy<-filter(oy,rdate>"2018-06-01 00:00:00" & rdate< "2018-09-30 00:00:00")
s.bf<-filter(bf,rdate>"2018-06-01 00:00:00" & rdate< "2018-09-30 00:00:00")
s.fb<-filter(fb,rdate>"2018-06-01 00:00:00" & rdate< "2018-09-30 00:00:00")
s.to3<-filter(to3,rdate>"2018-06-01 00:00:00" & rdate< "2018-09-30 00:00:00")
s.hmi2<-filter(hmi2,rdate>"2018-06-01 00:00:00" & rdate< "2018-09-30 00:00:00")

##Quartiles
q<-data.frame("site" = c("gbj","wh","oy","bf","fb","hmi2","to3"),"quantile"=NA,"decile"=NA, "max"=NA)
q[1,2]<-quantile(s.gbj$WTMP,0.75,type=1)
q[2,2]<-quantile(s.wh$WTMP,0.75,type=1)
q[3,2]<-quantile(s.oy$WTMP,0.75,type=1)
q[4,2]<-quantile(s.bf$WTMP,0.75,type=1)
q[5,2]<-quantile(s.fb$WTMP,0.75,type=1)
q[6,2]<-quantile(s.hmi2$WTMP,0.75,type=1)
q[7,2]<-quantile(s.to3$WTMP,0.75,type=1)

q[1,3]<-quantile(s.gbj$WTMP,0.9,type=1)
q[2,3]<-quantile(s.wh$WTMP,0.9,type=1)
q[3,3]<-quantile(s.oy$WTMP,0.9,type=1)
q[4,3]<-quantile(s.bf$WTMP,0.9,type=1)
q[5,3]<-quantile(s.fb$WTMP,0.9,type=1)
q[6,3]<-quantile(s.hmi2$WTMP,0.9,type=1)
q[7,3]<-quantile(s.to3$WTMP,0.9,type=1)

q[1,4]<-s.gbj %>% group_by(WTMP) %>% summarise(Value = max(WTMP))
q[2,4]<-s.wh %>% group_by(WTMP) %>% summarise(Value = max(WTMP))
q[3,4]<-s.oy %>% group_by(WTMP) %>% summarise(Value = max(WTMP))
q[4,4]<-s.bf %>% group_by(WTMP) %>% summarise(Value = max(WTMP))
q[5,4]<-s.fb %>% group_by(WTMP) %>% summarise(Value = max(WTMP))
q[6,4]<-s.hmi2 %>% group_by(WTMP) %>% summarise(Value = max(WTMP))
q[7,4]<-s.to3 %>% group_by(WTMP) %>% summarise(Value = max(WTMP))


#MEAN SST
temp<-rbind(pac2,atll) #temperature data for all

means<-data.frame(with(temp,tapply(WTMP,site,mean)))

temp$means<-ifelse(temp$site=="bf",means[1,1], ifelse(temp$site=="oy",means[5,1],ifelse(temp$site=="wh",means[7,1],ifelse(temp$site=="gbj",means[3,1],ifelse(temp$site=="to3",means[6,1],ifelse(temp$site=="hmi2",means[4,1],ifelse(temp$site=="fb",means[2,1],NA))))))) #stitching temperature data to sites

temp$site_ID<-ifelse(temp$site=="bf","BF", ifelse(temp$site=="oy","OY",ifelse(temp$site=="wh","WH",ifelse(temp$site=="gbj","GB",ifelse(temp$site=="to3","TO",ifelse(temp$site=="hmi2","HM",ifelse(temp$site=="fb","FB",NA))))))) 




#stitching mean temperatures to lt50s
lt50s$mean.sst<-ifelse(lt50s$site=="BF",means[1,1], ifelse(lt50s$site=="OY",means[5,1],ifelse(lt50s$site=="WH",means[7,1],ifelse(lt50s$site=="GB",means[3,1],ifelse(lt50s$site=="TO",means[6,1],ifelse(lt50s$site=="HM",means[4,1],ifelse(lt50s$site=="FB",means[2,1],NA))))))) 
#the same

 
##SUMMER MEAN SST

summer.temp<-filter(temp,rdate>"2018-06-01 00:00:00" & rdate< "2018-09-30 00:00:00")
s.mean<-data.frame(with(summer.temp,tapply(WTMP,site,mean)))

lt50s$s.mean.sst<-ifelse(lt50s$site=="BF",s.mean[1,1], ifelse(lt50s$site=="OY",s.mean[5,1],ifelse(lt50s$site=="WH",s.mean[7,1],ifelse(lt50s$site=="GB",s.mean[3,1],ifelse(lt50s$site=="TO",s.mean[6,1],ifelse(lt50s$site=="HM",s.mean[4,1],ifelse(lt50s$site=="FB",s.mean[2,1],NA)))))))

##UPPER QUARTILE SUMMER MEAN SST


lt50s$q.mean.sst<-ifelse(lt50s$site=="BF",q[4,2], ifelse(lt50s$site=="OY",q[3,2],ifelse(lt50s$site=="WH",q[2,2],ifelse(lt50s$site=="GB",q[1,2],ifelse(lt50s$site=="TO",q[7,2],ifelse(lt50s$site=="HM",q[6,2],ifelse(lt50s$site=="FB",q[5,2],NA)))))))

##UPPER 90th PERCENTILE SUMMER MEAN SST


lt50s$t.mean.sst<-ifelse(lt50s$site=="BF",q[4,3], ifelse(lt50s$site=="OY",q[3,3],ifelse(lt50s$site=="WH",q[2,3],ifelse(lt50s$site=="GB",q[1,3],ifelse(lt50s$site=="TO",q[7,3],ifelse(lt50s$site=="HM",q[6,3],ifelse(lt50s$site=="FB",q[5,3],NA)))))))

lt50s$oce<-ifelse(lt50s$site=="GB","a",ifelse(lt50s$site=="WH","a",ifelse(lt50s$site=="OY","a",ifelse(lt50s$site=="BF","a",ifelse(lt50s$site=="FB","a",ifelse(lt50s$site=="TO","p",ifelse(lt50s$site=="HM","p",NA))))))) #assigning ocean

##max
lt50s$max<-ifelse(lt50s$site=="BF",q[4,4], ifelse(lt50s$site=="OY",q[3,4],ifelse(lt50s$site=="WH",q[2,4],ifelse(lt50s$site=="GB",q[1,4],ifelse(lt50s$site=="TO",q[7,4],ifelse(lt50s$site=="HM",q[6,4],ifelse(lt50s$site=="FB",q[5,4],NA)))))))

#lat
lt50s$lat<-ifelse(lt50s$site=="BF",34.819,ifelse(lt50s$site=="FB",32.660525,ifelse(lt50s$site=="GB",43.089589,ifelse(lt50s$site=="HM",40.849448,ifelse(lt50s$site=="OY",37.288562,ifelse(lt50s$site=="TO",38.12805,ifelse(lt50s$site=="WH",41.57687,NA)))))))
lt50s$P<-ifelse(lt50s$P=="I",1,ifelse(lt50s$P=="II",2,NA))
lt50s<-filter(lt50s,run_ID!="2")
plot1<-ggplot(lt50s,aes(x=lat,y=LT50))+geom_point()+facet_wrap(acc~P)+geom_smooth(se=F,method="lm")+xlab("latitude,outlier_deaths_included")


```


```{r,echo=F,warning=F}
atll2<-rbind(fb,bf,oy,wh,gbj)
pac3<-rbind(to3,hmi2)
all<-rbind(atll2,pac3)
all$date<-as.Date(all$rdate,format="%Y-%m-%d %H:%M:%OS")

all$site<-factor(all$site,levels=c("gbj","wh","hmi2","to3","oy","bf","fb"))

#daily mean at each site
all.day <- all %>% 
  mutate(day=format(date,"%d"),month = format(date, "%m"),year=format(date,"%Y")) %>%
  group_by(site)%>%
  group_by(day,month,year,site) %>% 
  dplyr::summarise(mean = mean(WTMP), min = min(WTMP), max = max(WTMP))

  all.day$date<-paste(all.day$year,all.day$month,all.day$day, sep="-") %>% ymd() %>% as.Date()

```
```{r,include=F}
##which dates have a mean above 24C?


row.has.na <- apply(bf, 1, function(x){any(is.na(x))})
bf <- bf[!row.has.na,]
dfX <- xts(bf$WTMP, as.Date(bf$rdate))
bf_24<-apply.daily(dfX, max)

row.has.na <- apply(fb, 1, function(x){any(is.na(x))})
fb <- fb[!row.has.na,]
dfX <- xts(fb$WTMP, as.Date(fb$rdate))
fb_24<-apply.daily(dfX, mean)

row.has.na <- apply(oy, 1, function(x){any(is.na(x))})
oy <- oy[!row.has.na,]
dfX <- xts(oy$WTMP, as.Date(oy$rdate))
oy_24<-as.data.frame(apply.daily(dfX, mean))

row.has.na <- apply(wh, 1, function(x){any(is.na(x))})
wh <- wh[!row.has.na,]
dfX <- xts(wh$WTMP, as.Date(wh$rdate))
wh_24<-apply.daily(dfX, mean)

row.has.na <- apply(gbj, 1, function(x){any(is.na(x))})
gbj <- gbj[!row.has.na,]
dfX <- xts(gbj$WTMP, as.Date(gbj$rdate))
gb_24<-apply.monthly(dfX, mean)

oy_24
bf_24
fb_24
wh_24
gb_24

```

## Comparison of LT50 by four target environmental metrics. 

```{r,echo=F}

summary(lt50s)

ggplot(lt50s,aes(x=mean.sst,y=LT50, group=interaction(lat),color=site))+geom_point()+facet_wrap(acc~P)+geom_smooth(se=F,method="lm",aes(group=1))+xlab("meansst")

ggplot(lt50s,aes(x=s.mean.sst,y=LT50, group=interaction(lat),color=site))+geom_point()+facet_wrap(acc~P)+geom_smooth(se=F,method="lm",aes(group=1))+xlab("summer_meansst")

ggplot(lt50s,aes(x=q.mean.sst,y=LT50, group=interaction(site),color=site))+geom_point()+facet_wrap(acc~P)+geom_smooth(se=F,method="lm",aes(group=1))+xlab("quart.mean.sst")

ggplot(lt50s,aes(x=t.mean.sst,y=LT50, group=interaction(site),color=site))+geom_point()+facet_wrap(acc~P)+geom_smooth(se=F,method="lm",aes(group=1))+xlab("quart.mean.sst")

```

We can see that the variance about the lines of best fit and the slope of the lines do differ amongst the different environmental metrics. Therefore, we will have to include them in our model selection framework to determin which best drives patterns in LT50. 

## Competing environmental data/ $CT_{max}$ models

# Comparing lt50s among acclimations 

```{r,echo=F,warning=F}
c<-cor(lt50s[,c(8:11,13:16)])
chart.Correlation(c)
prot1<-filter(lt50s,P=="1")
mods<-list(
"null"=lm(LT50~1,prot1),
"lat"=lm(LT50~lat,prot1),#latitude
"mean"=lm(LT50~mean.sst,prot1),#mean yearly sst
"s.mean"=lm(LT50~s.mean.sst,prot1),#mean summer sst (06/01 - 09/01)
"q.mean"=lm(LT50~q.mean.sst,prot1),#upper 25% quartile of summer mean sst
"t.mean"=lm(LT50~t.mean.sst,prot1),#upper 10th percentile of summer mean sst
"acc"=lm(LT50~acc,prot1),
"lat.acc"=lm(LT50~lat+acc,prot1),
"mean.acc"=lm(LT50~mean.sst+acc,prot1),
"s.mean.acc"=lm(LT50~s.mean.sst,prot1),
"q.mean.acc"=lm(LT50~q.mean.sst+acc,prot1),
"t.mean.acc"=lm(LT50~t.mean.sst+acc,prot1),
"lat*acc"=lm(LT50~lat*acc,prot1),
"mean*acc"=lm(LT50~mean.sst*acc,prot1),
"s.mean*acc"=lm(LT50~s.mean.sst*acc,prot1),
"q.mean*acc"=lm(LT50~q.mean.sst*acc,prot1),
"t.mean*acc"=lm(LT50~t.mean.sst*acc,prot1),
"max"=lm(LT50~max,prot1),
"maxacc"=lm(LT50~max+acc,prot1),
"max*acc"=lm(LT50~max*acc,prot1))


aictab(mods)
mod1=(mods$'max*acc')#best fit model
summary(mod1)


by_acc<-group_by(lt50s,acc)
a20<-filter(prot1,acc=="20")
do(by_acc,
  broom::glance(
  lm(LT50~max,data=prot1)
  ))
summary(lm(LT50~max,data=a20))


#plot(mod1)
E1<-resid(mod1,type="pearson")
N<-nrow(prot1)
p<-length(coef(mod1))
sum(E1^2) / (N-p)
#no overdispersion


#make models for both acclimation temperatures
#models1<-dlply(prot1,"acc",.fun=make_model)
#predvals.acc<-ldply(models1,.fun=predictvals,xvar="t.mean.sst",yvar="LT50")

fig5<-ggplot(prot1,aes(x=max,y=LT50,color=acc,shape=oce,fill=acc))+geom_point(size=3)+theme_classic()+scale_color_manual(labels=c("20°C","24°C"),name="Acclimation",values=c("blue","red"))+theme_classic()+xlab("Habitat Temperature (SST °C")+scale_fill_manual(labels=c("20°C","24°C"),name="Acclimation",values=c("blue","red"))+scale_shape_manual(labels=c("Atlantic","Pacific"),name="Ocean",values=c(21,24))+
  theme(text=element_text(family="arial",size=22))+labs(y=expression(paste("LT"[50],  " (°C)")), x=expression(paste("T"[hab], " (SST °C)")))+scale_x_continuous(breaks=c(20,22,24,26,28,30,32,34),limits=c(20,34))+scale_y_continuous(breaks=c(35,36,37,38,39,40,41),limits=c(35,41))+  
  geom_segment(aes(x = 21.8, xend = 33.56934, y = 37.97841       + 0.04861      *21.8	, yend = 37.97841       + 0.04861       *33.56934	),size=1.5,color='blue')+
geom_segment(aes(x = 21.8, xend = 33.56934, y = (37.97841+4.15862)   + (0.04861-0.18156 )  *21.8	, yend = (37.97841+4.15862)   + (0.04861-0.18156 )   *33.56934	),size=1.5,color='red')
  
fig5

#summary stats

acc20a<-filter(lt50s,P=="1")
acc20a<-filter(acc20a,acc=="20")
acc24a<-filter(lt50s,P=="2")
acc24a<-filter(acc24a,acc=="24")
mean(acc20a$LT50)
mean(acc24a$LT50)
sqrt(var(acc20a$LT50))
sqrt(var(acc24a$LT50))

```


## Warming Tolerance

Deutsch et al. (2008 PNAS) discuss "Warming Tolerance," which they define as WT = $CT_{max}$ -$T_{hab}$, or the difference between thermal tolerance and temperature of the habitat. $T_{hab}$ is usually measured as the mean temperature. The smaller the warming tolerance is, they more likely that population is to be vulnerable to climate change. I will calculate a mean WT, an upper 90th percentile wt, and Wt with habitat maximum.
```{r,include=T,echo=F, warning=F}
lt50s$wt<-lt50s$LT50-lt50s$mean.sst
lt50s$wt.t<-lt50s$LT50-lt50s$t.mean.sst
prot1$wt.t<-prot1$LT50-prot1$t.mean.sst
prot1$wt.max<-prot1$LT50-prot1$max
prot1$lat<-as.numeric(prot1$lat)
prot1$acc<-as.factor(prot1$acc)

##Warming tolerance using Max as Thab
warm.max<-list(
"a.1"=lm(wt.max~lat,prot1),
"b.1"=lm(wt.max~mean.sst,prot1),
"c.1"=lm(wt.max~s.mean.sst,prot1),
"d.1"=lm(wt.max~q.mean.sst,prot1),
"e.1"=lm(wt.max~t.mean.sst,prot1),
"f.1"=lm(wt.max~lat+acc,prot1),
"g.1"=lm(wt.max~mean.sst+acc,prot1),
"h.1"=lm(wt.max~s.mean.sst+acc,prot1),
"i.1"=lm(wt.max~q.mean.sst+acc,prot1),
"j.1"=lm(wt.max~t.mean.sst+acc,prot1),
"k.1"=lm(wt.max~acc,prot1),
"l.1"=lm(wt.max~lat*acc,prot1),
"m.1"=lm(wt.max~mean.sst*acc,prot1),
"n.1"=lm(wt.max~s.mean.sst*acc,prot1),
"o.1"=lm(wt.max~q.mean.sst*acc,prot1),
"p.1"=lm(wt.max~t.mean.sst*acc,prot1),
"q.2"=lm(wt.max~max,prot1),
"r.2"=lm(wt.max~max+acc,prot1),
"s.2"=lm(wt.max~max*acc,prot1))

aictab(warm.max)
summary(s.2)#best fit model
summary(lm(wt.max~max*acc,prot1))

ggplot(prot1,aes(x=max,y=wt.max,fill=acc,group=lat,shape=oce,color=acc))+theme_classic()+theme(text=element_text(family="sanserif",size=22))+labs(y="Warming Tolerance",x="Latitude")+scale_fill_manual(labels=c("20°C","24°C"),name="Acclimation", values=c('blue','red'))+scale_color_manual(labels=c("20°C","24°C"),name="Acclimation",values=c('blue','red'))+scale_shape_manual(name="Ocean",labels = c("Atlantic","Pacific"),values=c(21,24))+geom_segment(aes(x = 21.8, xend = 33.56934, y = 37.97841 -(0.95139 *21.8), yend = 37.97841 -(0.95139 *33.56934)	),size=1.5,color='blue')+geom_point(size=3)+scale_x_continuous(name=expression(paste("T"[hab], " (SST °C)")),limits=c(20,34),breaks=c(20,22,24,26,28,30,32,34))+
  geom_segment(aes(x = 21.8, xend = 33.56934, y = (37.97841+4.15862 ) +((-0.95139-0.18156) *21.8), yend = (37.97841+4.15862 ) +((-0.95139-0.18156) *33.56934)	),size=1.5,color='red')+
  scale_y_continuous(name="Warming Tolerance (°C)",limits=c(0,20),breaks=c(0,4,8,12,16,20)) 


```

